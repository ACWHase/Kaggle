{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"},{"sourceId":12157156,"sourceType":"datasetVersion","datasetId":7656641}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing packages\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.tseries.offsets import DateOffset\n\n# Plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sklearn\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Models\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom lightgbm.callback import early_stopping\n\n# Optuna for hyperparameter optimization\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:46.789225Z","iopub.execute_input":"2025-06-18T06:46:46.789635Z","iopub.status.idle":"2025-06-18T06:46:46.794357Z","shell.execute_reply.started":"2025-06-18T06:46:46.789570Z","shell.execute_reply":"2025-06-18T06:46:46.793469Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Objective**\n\nThis notebook is designed to forecast store sales 16 days into the future. The data holds sales data per day collected between 2013 and July 2017. It is constructed as follows: each row gives the sales and number of items on promotion,on a specific day for a specific store and in a specific product family. Other data sources provide information about national, regional or local holidays, location and properties of the stores, oil prices, and total transactions per day per store.","metadata":{}},{"cell_type":"markdown","source":"**Approach**\n\nThe task involves forecasting different time-series, that is, for each pair (store number, product family) we have a distinct time series. However, we will use machine-learning models to learn the dynamics of these time series together, since we believe that there are similarities between the dynamics. In order to enable the models to learn patterns that apply across different pairs of (store number, product family), we choose to feed our model with the entire data, using store number and family as features.\n\nWe will train different models to predict the sales for each of the 16 days in the test file. We will use the last date of the train file as the prediction origin.","metadata":{}},{"cell_type":"markdown","source":"**Data ingestion and preparation**\n\nIn the following, we read the different data sources and prepare them. The following preliminary processing is handled:\n1. converting the 'date' field to datetime format\n2. default values are included for missing dates (no sales on Christmas)\n3. missing oil prices are filled through linear interpolation.\n4. missing transactions entries are filled by sales / average sales per transaction per store.\n5. holidays and events are binned (for example, all football events are binned together), and are loaded to the data frame for stores for which they are geographically relevant.\n\nAll these processed data sources are merged into a full data frame that contains the underlying information, from which other features are extracted.","metadata":{}},{"cell_type":"code","source":"# Reading in provided source data\n\ntrain_df = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\",index_col='id')\nholidays_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\noil_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\nstores_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\ntransactions_df = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\ntest_df = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\",index_col='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:46.795652Z","iopub.execute_input":"2025-06-18T06:46:46.795875Z","iopub.status.idle":"2025-06-18T06:46:48.444306Z","shell.execute_reply.started":"2025-06-18T06:46:46.795857Z","shell.execute_reply":"2025-06-18T06:46:48.443663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correct date format\n\ntrain_df['date'] = pd.to_datetime(train_df['date'])\nholidays_df['date'] = pd.to_datetime(holidays_df['date'])\noil_df['date'] = pd.to_datetime(oil_df['date'])\ntransactions_df['date'] = pd.to_datetime(transactions_df['date'])\ntest_df['date'] = pd.to_datetime(test_df['date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:48.445980Z","iopub.execute_input":"2025-06-18T06:46:48.446219Z","iopub.status.idle":"2025-06-18T06:46:48.688760Z","shell.execute_reply.started":"2025-06-18T06:46:48.446198Z","shell.execute_reply":"2025-06-18T06:46:48.687832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find missing dates in train_df (Christmas). Add entries for these dates with sales of 0 for each store_nbr, family.\n\nmissing_dates = pd.date_range(start=train_df['date'].min(), end=train_df['date'].max()).difference(train_df['date'])\ntemporary_df = train_df[(train_df['date']>='2017-02-03') & (train_df['date']<=pd.to_datetime('2017-02-03')+DateOffset(days=len(missing_dates)-1))].copy()\ntemporary_df[['sales','onpromotion']] = 0\nfor i in range(len(missing_dates)):\n    temporary_df.loc[temporary_df['date']==pd.to_datetime('2017-02-03')+DateOffset(days=i),'date'] = missing_dates[i]\ntrain_df =  pd.concat([train_df, temporary_df], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:48.690191Z","iopub.execute_input":"2025-06-18T06:46:48.690511Z","iopub.status.idle":"2025-06-18T06:46:48.821328Z","shell.execute_reply.started":"2025-06-18T06:46:48.690481Z","shell.execute_reply":"2025-06-18T06:46:48.820662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatenate train_df and test_df. Sort by date.\nfull_df = pd.concat([train_df, test_df], axis = 0)\nfull_df = full_df.sort_values(by=['date','store_nbr','family'],ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:48.822197Z","iopub.execute_input":"2025-06-18T06:46:48.822420Z","iopub.status.idle":"2025-06-18T06:46:49.241167Z","shell.execute_reply.started":"2025-06-18T06:46:48.822400Z","shell.execute_reply":"2025-06-18T06:46:49.240230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Interpolate oil prices linearly where missing.\ncalendar_df = pd.DataFrame(pd.date_range(start='2013-01-01', end='2017-08-15'), columns = ['date'])\noil_df = pd.merge(left=calendar_df,right=oil_df,on='date',how='left')\noil_df['dcoilwtico'] = oil_df['dcoilwtico'].interpolate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:49.241964Z","iopub.execute_input":"2025-06-18T06:46:49.242194Z","iopub.status.idle":"2025-06-18T06:46:49.250473Z","shell.execute_reply.started":"2025-06-18T06:46:49.242174Z","shell.execute_reply":"2025-06-18T06:46:49.249660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preparing transaction data\n\n# Transaction data is missing for some dates and stores. Use sales data and average sales per transaction to estimate transaction count.\n\n# Calculate sum of sales per store\ntransactions_df = pd.merge(left=transactions_df,right=pd.DataFrame(train_df.groupby(['date','store_nbr'])['sales'].sum()).reset_index(),on=['date', 'store_nbr'],how='outer')\n\n# Calculate sales per transaction (per day and store)\ntransactions_df['sales_per_transaction'] = transactions_df['sales'] / transactions_df['transactions']\n\n# Calculate average sales per transaction per store\nstore_ratio = transactions_df.groupby('store_nbr')['sales_per_transaction'].mean().to_dict()\n\n# Fill missing transactions with sales / average sales per transaction per store.\ntransactions_df['transactions'] = transactions_df['transactions'].fillna(transactions_df['sales']/transactions_df['store_nbr'].map(store_ratio))\ntransactions_df = transactions_df[['date', 'store_nbr', 'transactions']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:49.251449Z","iopub.execute_input":"2025-06-18T06:46:49.251761Z","iopub.status.idle":"2025-06-18T06:46:49.411819Z","shell.execute_reply.started":"2025-06-18T06:46:49.251724Z","shell.execute_reply":"2025-06-18T06:46:49.411102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preparing holiday data\n\nnew_holidays_df = holidays_df[holidays_df['transferred']==False]\nnew_holidays_df = new_holidays_df.groupby(['date','locale','locale_name']).sum()\nnew_holidays_df = new_holidays_df.reset_index()\n\n# Remove transferred holidays as they resemble workdays\nnew_holidays_df = holidays_df[holidays_df['transferred']==False]\n\n# Binning football holidays (before anticipated holidays to remove '-')\nnew_holidays_df.loc[(new_holidays_df['description'].str.contains('futbol')), 'description'] = 'Futbol'\n\n# Removing anticipated holidays because we will use future features\nnew_holidays_df = new_holidays_df.loc[new_holidays_df['description'].str.find('-')==-1]\n\n# Binning local holidays\nnew_holidays_df.loc[(new_holidays_df['locale']=='Local') & (new_holidays_df['description'].str.contains('Fundacion')), 'description'] = 'Fundacion'\nnew_holidays_df.loc[(new_holidays_df['locale']=='Local') & (new_holidays_df['description'].str.contains('Cantonizacion')), 'description'] = 'Cantonizacion'\nnew_holidays_df.loc[(new_holidays_df['locale']=='Local') & (new_holidays_df['description'].str.contains('Independencia')), 'description'] = 'Independencia'\n\n# Binning regional holidays\nnew_holidays_df.loc[(new_holidays_df['locale']=='Regional') & (new_holidays_df['description'].str.contains('Provincializacion')), 'description'] = 'Provincializacion'\n\n# Binning national holidays\nnew_holidays_df.loc[(new_holidays_df['locale']=='National') & (new_holidays_df['description'].str.contains('Terremoto')), 'description'] = 'Earthquake'\nnew_holidays_df.loc[(new_holidays_df['locale']=='National') & (new_holidays_df['description'].str.contains('Recupero')), 'description'] = 'Work Day'\nnew_holidays_df['description'] = new_holidays_df['description'].str.replace('Traslado ','')\n\n# Removing past holidays as we use lagged features\nnew_holidays_df = new_holidays_df.loc[new_holidays_df['description'].str.find('+')==-1]\n\n# Joining descriptions if more than one holiday applies in same location\ncheck_df = new_holidays_df.groupby(['date','locale','locale_name'])['description'].apply(', '.join).reset_index()\nnew_holidays_df = new_holidays_df.drop('description', axis=1)\nnew_holidays_df = new_holidays_df.merge(check_df, on=['date','locale','locale_name'], how='left')\nnew_holidays_df = new_holidays_df.drop_duplicates(subset=['date','locale','locale_name'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:49.413942Z","iopub.execute_input":"2025-06-18T06:46:49.414152Z","iopub.status.idle":"2025-06-18T06:46:49.450418Z","shell.execute_reply.started":"2025-06-18T06:46:49.414133Z","shell.execute_reply":"2025-06-18T06:46:49.449831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preparing store data\n\n# Renaming type in stores_df to avoid any confusion with holiday types.\nstores_df = stores_df.rename(columns={'type':'store_type'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:49.451851Z","iopub.execute_input":"2025-06-18T06:46:49.452072Z","iopub.status.idle":"2025-06-18T06:46:49.455617Z","shell.execute_reply.started":"2025-06-18T06:46:49.452042Z","shell.execute_reply":"2025-06-18T06:46:49.454816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merging dataframes\n\nfull_df = pd.merge(left=full_df,right=stores_df,on='store_nbr',how='left')\nfull_df = pd.merge(left=full_df,right=oil_df,on='date',how='left')\nfull_df = pd.merge(left=full_df,right=transactions_df,on=['date','store_nbr'],how='left')\nfull_df = pd.merge(left=full_df,right=new_holidays_df.loc[new_holidays_df['locale']=='National',['date','description','type']],\n                                    on='date',how='left')\nfull_df = full_df.rename(columns={'description':'national_holiday_description','type':'national_holiday_type'})\nfull_df = pd.merge(left=full_df,right=new_holidays_df.loc[new_holidays_df['locale']=='Regional',['date','locale_name','description','type']],\n                                    left_on=['date','state'],right_on=['date','locale_name'],how='left')\nfull_df = full_df.rename(columns={'description':'regional_holiday_description','type':'regional_holiday_type'})\nfull_df = full_df.drop(axis='columns',labels='locale_name')\nfull_df = pd.merge(left=full_df,right=new_holidays_df.loc[new_holidays_df['locale']=='Local',['date','locale_name','description','type']],\n                                    left_on=['date','city'],right_on=['date','locale_name'],how='left')\nfull_df = full_df.rename(columns={'description':'local_holiday_description','type':'local_holiday_type'})\nfull_df = full_df.drop(axis='columns',labels='locale_name')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:49.456351Z","iopub.execute_input":"2025-06-18T06:46:49.456565Z","iopub.status.idle":"2025-06-18T06:46:57.261648Z","shell.execute_reply.started":"2025-06-18T06:46:49.456547Z","shell.execute_reply":"2025-06-18T06:46:57.260883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Place default string value if there is no holiday.\nholiday_features = ['national_holiday_description','national_holiday_type',\n                'regional_holiday_description','regional_holiday_type',\n                'local_holiday_description','local_holiday_type']\nfull_df[holiday_features] = full_df[holiday_features].fillna('No holiday')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:57.262456Z","iopub.execute_input":"2025-06-18T06:46:57.262739Z","iopub.status.idle":"2025-06-18T06:46:58.781239Z","shell.execute_reply.started":"2025-06-18T06:46:57.262716Z","shell.execute_reply":"2025-06-18T06:46:58.780247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After processing, we verify that each possible triplet (date, store number, family) appears exactly once in the data frame.","metadata":{}},{"cell_type":"code","source":"# Sanity check: After processing, each pair of store_nbr, family has exactly one entry for each date.\n\n# Each possible triple appears, because the number of possible distinct triples == number of appearing distinct triples.\n\nx = len(set(list(zip(full_df['date'], full_df['store_nbr'], full_df['family']))))\n\nprint(full_df['date'].nunique()*full_df['store_nbr'].nunique()*full_df['family'].nunique() == x)\n\n# Each triple appears exaxtly once.\n\nprint(x == len(list(zip(full_df['date'], full_df['store_nbr'], full_df['family']))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:46:58.782163Z","iopub.execute_input":"2025-06-18T06:46:58.782518Z","iopub.status.idle":"2025-06-18T06:47:15.166549Z","shell.execute_reply.started":"2025-06-18T06:46:58.782466Z","shell.execute_reply":"2025-06-18T06:47:15.165657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Feature engineering**\n\nWe will use the last date of the train file (2017-08-15) as the prediction origin.\nSo all features for our models need to be available in the rows for this date.\n\nWe create the following features:\n1. day of the month\n2. month of year, sine and cosine with period one year to capture seasonality\n3. year\n4. day of week, sine and cosine with period one week to capture seasonality\n5. day of payment cycle, sine and cosine with period two weeks to capture seasonality\n6. past sales shifted (autoregressive dynamics)\n7. moving average of sales with window sizes of one week, one month\n\nWe also create the following future features (future data that is known at the forecast origin).\nThere is some danger of leakage, but we assume promotions are known two weeks in advance:\n\n8. holidays in the prediction period\n9. promotions in the prediction period\n10. moving average of onpromotion over the prediction period","metadata":{}},{"cell_type":"code","source":"prediction_date = train_df['date'].max()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:47:15.167464Z","iopub.execute_input":"2025-06-18T06:47:15.167798Z","iopub.status.idle":"2025-06-18T06:47:15.179078Z","shell.execute_reply.started":"2025-06-18T06:47:15.167767Z","shell.execute_reply":"2025-06-18T06:47:15.178508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Time features\n\nfull_df['day'] = full_df['date'].dt.day\nfull_df['month'] = full_df['date'].dt.month\nfull_df['year'] = full_df['date'].dt.year\n\nfull_df['day_of_week'] = full_df['date'].dt.day_of_week\nfull_df['sin_day_of_week'] = np.sin(2*np.pi*(full_df['day_of_week']/7))\nfull_df['cos_day_of_week'] = np.cos(2*np.pi*(full_df['day_of_week']/7))\n\nfull_df['sin_month_of_year'] = np.sin(2*np.pi*(full_df['date'].dt.month/12))\nfull_df['cos_month_of_year'] = np.cos(2*np.pi*(full_df['date'].dt.month/12))\n\nfull_df['day_of_pay_cycle'] = (full_df['day'] - 15)*(full_df['day'] >= 15)+(full_df['day']+1)*(full_df['day'] < 15)\nfull_df['sin_day_of_pay_cycle'] = np.sin(2*np.pi*(full_df['day_of_pay_cycle']/15))\nfull_df['cos_day_of_pay_cycle'] = np.cos(2*np.pi*(full_df['day_of_pay_cycle']/15))\n\n# Lag features\n\n# Sales from every day of the last week\n\nshifted_sales = {\n    f'sales_lag_{i}': full_df.groupby(['store_nbr', 'family'])['sales'].shift(i) \n    for i in range(1, 8)\n}\n\nfull_df = pd.concat([full_df, pd.DataFrame(shifted_sales)], axis=1)\n\n# Sales on the same day of the month, from last month\n\nfull_df['next_month'] = full_df['date']+ DateOffset(months=1)\nfull_df  = pd.merge(left=full_df,right=full_df[['next_month','store_nbr','family', 'sales']], \n                               left_on=['date','store_nbr', 'family'], \n                               right_on=['next_month','store_nbr','family'], \n                               how='left')\nfull_df = full_df.rename(columns={'sales_x':'sales','sales_y':'sales_last_month'})\nfull_df = full_df.drop(labels=['next_month_x','next_month_x'], axis = 1)\n\n# Rolling averages of sales data\n\nfull_df['MA_last_week'] = full_df.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=7).mean())\nfull_df['MA_last_month'] = full_df.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=30).mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:47:15.179814Z","iopub.execute_input":"2025-06-18T06:47:15.180008Z","iopub.status.idle":"2025-06-18T06:47:27.045268Z","shell.execute_reply.started":"2025-06-18T06:47:15.179990Z","shell.execute_reply":"2025-06-18T06:47:27.044624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating future features.\n\nholiday_features = ['national_holiday_description','national_holiday_type',\n                    'regional_holiday_description','regional_holiday_type',\n                    'local_holiday_description','local_holiday_type']\nfuture_features = holiday_features + ['onpromotion']\n\nfull_df = full_df.drop(columns=[f'{feature}_step_{i}' for feature in future_features for i in range(0, 17)], errors='ignore')\n\nshifted_data = {\n    f'{feature}_step_{i}': full_df.groupby(['store_nbr', 'family'])[f'{feature}'].shift(-i) \n    for feature in future_features for i in range(0, 17)\n}\n\nfull_df = pd.concat([full_df, pd.DataFrame(shifted_data)], axis=1)\n\n# Rolling average of future promotions\nindexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=16)\nfull_df['MA_future_promotions'] = full_df.groupby(['store_nbr', 'family'])['onpromotion'].transform(lambda x: x.rolling(window=indexer).mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:47:27.046096Z","iopub.execute_input":"2025-06-18T06:47:27.046421Z","iopub.status.idle":"2025-06-18T06:48:09.790391Z","shell.execute_reply.started":"2025-06-18T06:47:27.046390Z","shell.execute_reply":"2025-06-18T06:48:09.789730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Future features are also available with column name suffix '_step_0'.\n\nfull_df = full_df.drop(columns=future_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:09.791132Z","iopub.execute_input":"2025-06-18T06:48:09.791360Z","iopub.status.idle":"2025-06-18T06:48:13.879374Z","shell.execute_reply.started":"2025-06-18T06:48:09.791340Z","shell.execute_reply":"2025-06-18T06:48:13.878676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Here we make the distinction between numerial and categorical features.\n\n# Categorical features are \n\nfuture_categorical_features = [f'{feature}_step_{i}' for feature in holiday_features for i in range(0, 17)]\ncategorical_features = ['store_nbr', 'family', 'day_of_week'] # 'city', 'state', 'store_type', 'cluster'\n\nfull_df[categorical_features] = full_df[categorical_features].astype('category')\nfull_df[future_categorical_features] = full_df[future_categorical_features].astype('category')\n\n\nsales_features = [f'sales_lag_{i}' for i in range(1, 8)]\nnumerical_features = ['dcoilwtico', 'transactions',  \n                      'day', 'month', 'year', \n                      'sin_day_of_week', 'cos_day_of_week', 'sin_month_of_year', 'cos_month_of_year', \n                      'day_of_pay_cycle', 'sin_day_of_pay_cycle', 'cos_day_of_pay_cycle', \n                      'sales_last_month', 'MA_last_week', 'MA_last_month',\n                      'MA_future_promotions'] + sales_features\n\nfeature_list = []\n\n# For each day we will train a different models.\n# To only use the most relevant features for each model, We create a list, that stores in position \n\nfor i in range(1, 17):\n    feature_list.append(numerical_features + categorical_features \n                        + [f'{feature}_step_{j}' for feature in holiday_features for j in range(i-1, min(i+5,17))]\n                        + [f'onpromotion_step_{i}'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:13.880101Z","iopub.execute_input":"2025-06-18T06:48:13.880326Z","iopub.status.idle":"2025-06-18T06:48:37.789804Z","shell.execute_reply.started":"2025-06-18T06:48:13.880307Z","shell.execute_reply":"2025-06-18T06:48:37.788916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Examples\n\nstr_nbr = 7\nfml = 'BEVERAGES'\n\nsns.scatterplot(data = full_df[(full_df['store_nbr']== str_nbr)& (full_df['family']== fml)], x='sales_lag_1', y='sales')\n# sns.scatterplot(data = full_df[(full_df['store_nbr']== str_nbr)& (full_df['family']== fml)], x='sales_lag_7', y='sales')\n# sns.scatterplot(data = full_df[(full_df['store_nbr']== str_nbr)& (full_df['family']== fml)], x='dcoilwtico', y='sales')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:11:04.932300Z","iopub.execute_input":"2025-06-18T12:11:04.932645Z","iopub.status.idle":"2025-06-18T12:11:05.110433Z","shell.execute_reply.started":"2025-06-18T12:11:04.932620Z","shell.execute_reply":"2025-06-18T12:11:05.109644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The figure above motivates the use of the shifted sales. There is clearly a strong positive correlation between yesterday's sales and today's. This suggests that there might be an autoregressice component to the dynamics. This feature, and others, are further justified below when the F-score is computed.","metadata":{}},{"cell_type":"markdown","source":"**Target**\n\nThe objective is to forecast 16 days into the future. To this end, we will train 16 different models, one for each horizon. This is done by building a full dataframe which is composed of the features and target for all models. The features were discussed above, and the targets for each date are the sales in the 16 days that follow (negtively shifted sales).","metadata":{}},{"cell_type":"code","source":"# Create target data\n\nfull_df = full_df.drop(columns=[f'sales_step_{i}' for i in range(1, 17)], errors='ignore')\n\nshifted_data = {\n    f'sales_step_{i}': full_df.groupby(['store_nbr', 'family'])['sales'].shift(-i) \n    for i in range(1, 17)\n}\n\nfull_df = pd.concat([full_df, pd.DataFrame(shifted_data)], axis=1)\n\n# Take log on target\n\nfull_df['log_sales'] = np.log(1+full_df['sales'])\nfor i in range(1,17):\n    full_df[f'log_sales_step_{i}'] = np.log(1+full_df[f'sales_step_{i}'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:38.208491Z","iopub.execute_input":"2025-06-18T06:48:38.208770Z","iopub.status.idle":"2025-06-18T06:48:42.052883Z","shell.execute_reply.started":"2025-06-18T06:48:38.208747Z","shell.execute_reply":"2025-06-18T06:48:42.052185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter set to remove rows with missing features or targets\nsubset_na = [f'log_sales_step_{i}' for i in range(1,17)]+numerical_features\nX = full_df.dropna(axis=0,subset=subset_na)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:42.053730Z","iopub.execute_input":"2025-06-18T06:48:42.054023Z","iopub.status.idle":"2025-06-18T06:48:43.686287Z","shell.execute_reply.started":"2025-06-18T06:48:42.053992Z","shell.execute_reply":"2025-06-18T06:48:43.685562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Restrict data\n\n# X = X[X['year']>2014]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:43.690452Z","iopub.execute_input":"2025-06-18T06:48:43.690706Z","iopub.status.idle":"2025-06-18T06:48:43.693873Z","shell.execute_reply.started":"2025-06-18T06:48:43.690680Z","shell.execute_reply":"2025-06-18T06:48:43.693043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Target\ntarget_list = [f'log_sales_step_{i}' for i in range(1,17)]\ny = X[target_list]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:43.695814Z","iopub.execute_input":"2025-06-18T06:48:43.696056Z","iopub.status.idle":"2025-06-18T06:48:43.849718Z","shell.execute_reply.started":"2025-06-18T06:48:43.696036Z","shell.execute_reply":"2025-06-18T06:48:43.848906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train test split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:43.850469Z","iopub.execute_input":"2025-06-18T06:48:43.850767Z","iopub.status.idle":"2025-06-18T06:48:48.320821Z","shell.execute_reply.started":"2025-06-18T06:48:43.850746Z","shell.execute_reply":"2025-06-18T06:48:48.319859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Idea:\n# 1. Train different types of models\n# 2. Hyperparameter optimization\n# 3. After having chosen good hyperparameters, train each chosen model on a separate split.\n# 4. Combine (probably just average) models.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:48.321734Z","iopub.execute_input":"2025-06-18T06:48:48.321991Z","iopub.status.idle":"2025-06-18T06:48:48.324979Z","shell.execute_reply.started":"2025-06-18T06:48:48.321970Z","shell.execute_reply":"2025-06-18T06:48:48.324340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter optimization for xgb\n# Ideally, we would find the optimal hyperparameters for each model. This takes too much time.  Choose 7.\n\n# def objective_xgb(trial):\n#     params={\n#         'max_depth':trial.suggest_int('max_depth',6,15),\n#         'min_child_weight':trial.suggest_float('min_child_weight',1e-2,100,log=True),\n#         'subsample':trial.suggest_float('subsample',0.5,1),\n#         'learning_rate':trial.suggest_float('learning_rate',0.01,0.3,log=True),\n#         'colsample_bytree':trial.suggest_float('colsample_bytree',0.5,1)\n#     }\n#     model = XGBRegressor(\n#         **params,\n#         n_estimators=1000,\n#         early_stopping_rounds=5,\n#         eval_metric='rmse',\n#         tree_method='hist',\n#         enable_categorical = True\n#     )\n#     model.fit(\n#         X_train[feature_list[7]], \n#         y_train[f'log_sales_step_8'], \n#         eval_set=[(X_valid[feature_list[7]], y_valid[f'log_sales_step_8'])], \n#         verbose = 0)\n#     preds = model.predict(X_valid[feature_list[7]])\n#     return mean_squared_error(y_valid[f'log_sales_step_8'],preds)\n\n# study_xgb = optuna.create_study(direction='minimize')\n# study_xgb.optimize(objective_xgb,n_trials=50)\n# best_params_xgb = study_xgb.best_params\n# print(best_params_xgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:48.325757Z","iopub.execute_input":"2025-06-18T06:48:48.325958Z","iopub.status.idle":"2025-06-18T06:48:48.344772Z","shell.execute_reply.started":"2025-06-18T06:48:48.325931Z","shell.execute_reply":"2025-06-18T06:48:48.344150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Result of hyperparameter optimization for xgb\n\n# With 50 iterations:\n\n# best_params_xgb={\n#      'max_depth': 11, \n#      'min_child_weight': 5.716785274742769, \n#      'subsample': 0.9234688380913888, \n#      'learning_rate': 0.040487705693284505, \n#      'colsample_bytree': 0.6294748159852215\n#     }\n\n# With 35 iterations:\n\n# best_params_xgb={\n#         'max_depth':14,\n#         'min_child_weight':4.990640605482132,\n#         'subsample':0.8814379905410369,\n#         'learning_rate':0.03259546572,\n#         'colsample_bytree':0.8437365655068993\n#     }\n\n# Heuristic defaults:\n\nbest_params_xgb={\n    'max_depth': 8,\n    'min_child_weight': 5,\n    'subsample': 0.8,\n    'learning_rate': 0.05,\n    'colsample_bytree': 0.8\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:48.345556Z","iopub.execute_input":"2025-06-18T06:48:48.345851Z","iopub.status.idle":"2025-06-18T06:48:48.370760Z","shell.execute_reply.started":"2025-06-18T06:48:48.345822Z","shell.execute_reply":"2025-06-18T06:48:48.369989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter optimization for lgbm\n# Ideally, we would find the optimal hyperparameters for each model. This takes too much time.  Choose 7.\n\n# from lightgbm import LGBMRegressor\n# from lightgbm.callback import early_stopping\n\n# def objective_lgbm(trial):\n#     params={\n#         'num_leaves':trial.suggest_int('num_leaves',30,150),\n#         'min_child_weight':trial.suggest_float('min_child_weight',1e-3,100,log=True),\n#         'subsample':trial.suggest_float('subsample',0.5,1),\n#         'learning_rate':trial.suggest_float('learning_rate',0.01,0.3,log=True),\n#         'colsample_bytree':trial.suggest_float('colsample_bytree',0.5,1)\n#     }\n#     model = LGBMRegressor(\n#         **params,\n#         n_estimators=1000\n#     )\n#     model.fit(\n#         X_train[feature_list[7]],\n#         y_train[f'log_sales_step_8'],\n#         eval_set=[(X_valid[feature_list[7]], y_valid[f'log_sales_step_8'])],\n#         callbacks=[early_stopping(stopping_rounds=5)],\n#         eval_metric='rmse'\n#     )\n#     preds = model.predict(X_valid[feature_list[7]])\n#     return mean_squared_error(y_valid[f'log_sales_step_8'],preds)\n\n# study_lgbm = optuna.create_study(direction='minimize')\n# study_lgbm.optimize(objective_lgbm,n_trials=50)\n# best_params_lgbm = study_lgbm.best_params\n# print(best_params_lgbm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:48.371541Z","iopub.execute_input":"2025-06-18T06:48:48.371958Z","iopub.status.idle":"2025-06-18T06:48:48.388202Z","shell.execute_reply.started":"2025-06-18T06:48:48.371929Z","shell.execute_reply":"2025-06-18T06:48:48.387485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Result of hyperparameter optimization for lgbm\n\n# With 50 iterations: \n\n# best_params_lgbm={\n#     'num_leaves': 141, \n#      'min_child_weight': 0.0013941886951568947, \n#      'subsample': 0.892295408305398, \n#      'learning_rate': 0.10205166138848568, \n#      'colsample_bytree': 0.6582560601895524\n#     }\n\n# With 35 iterations:\n\n# best_params_lgbm={\n#         'num_leaves':141,\n#         'min_child_weight':0.02362221828066401,\n#         'subsample':0.6927354612523271,\n#         'learning_rate':0.10616785590747908,\n#         'colsample_bytree':0.7840718293546729\n#     }\n\n# Heuristic defaults:\n\nbest_params_lgbm={\n        'num_leaves':31,\n        'min_child_weight':5,\n        'subsample':0.8,\n        'learning_rate':0.05,\n        'colsample_bytree':0.8\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:48.388962Z","iopub.execute_input":"2025-06-18T06:48:48.389236Z","iopub.status.idle":"2025-06-18T06:48:48.409887Z","shell.execute_reply.started":"2025-06-18T06:48:48.389209Z","shell.execute_reply":"2025-06-18T06:48:48.409282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter optimization for cat\n# Ideally, we would find the optimal hyperparameters for each model. This takes too much time. Choose 7.\n\n# #pip install optuna-integration[catboost]\n# #from optuna.integration import CatBoostPruningCallback\n\n#def objective_cat(trial):\n#    params={\n#        'depth':trial.suggest_int('depth',4,10),\n#        'random_strength':trial.suggest_float('random_strength',1e-9,10,log=True),\n#        'bagging_temperature':trial.suggest_float('bagging_temperature',0,10),\n#        'learning_rate':trial.suggest_float('learning_rate',0.01,0.3,log=True),\n#        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True)\n#    }\n#    model = CatBoostRegressor(\n#        **params,\n#        loss_function='RMSE',\n#        task_type=\"GPU\",        # <-- enable GPU\n#        devices='0,1',\n#        iterations=500\n#    )\n#    # pruning_callback = CatBoostPruningCallback(trial, 'RMSE')\n#    model.fit(\n#        X_train[feature_list[7]],\n#        y_train[f'log_sales_step_8'],\n#        cat_features = list(set(categorical_features+future_categorical_features) & set(feature_list[7])),\n#        eval_set=(X_valid[feature_list[7]], y_valid[f'log_sales_step_8']),\n#        early_stopping_rounds=5,\n#        # callbacks=[pruning_callback],\n#        verbose = 50)\n#    preds = model.predict(X_valid[feature_list[7]])\n#    return mean_squared_error(y_valid[f'log_sales_step_8'],preds)\n\n# #study_cat = optuna.create_study(direction='minimize',pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n#study_cat = optuna.create_study(direction='minimize')\n#study_cat.optimize(objective_cat,n_trials=50)\n#best_params_cat = study_cat.best_params\n#print(best_params_cat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:48.410666Z","iopub.execute_input":"2025-06-18T06:48:48.410912Z","iopub.status.idle":"2025-06-18T06:48:48.429329Z","shell.execute_reply.started":"2025-06-18T06:48:48.410886Z","shell.execute_reply":"2025-06-18T06:48:48.428519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Result of hyperparameter optimization for catboost\n\n# With 50 iterations:\n\n# best_params_cat={\n#        'depth':9,\n#        'random_strength':0.05626974752238471,\n#        'bagging_temperature':0.09747368919292904,\n#        'learning_rate':0.236652999497516,\n#        'l2_leaf_reg':4.580022980524382}\n\n# Heuristic defaults:\n\nbest_params_cat={\n       'depth':8,\n       'random_strength':1,\n       'bagging_temperature':1,\n       'learning_rate':0.05,\n       'l2_leaf_reg':3}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:48.430177Z","iopub.execute_input":"2025-06-18T06:48:48.430434Z","iopub.status.idle":"2025-06-18T06:48:48.450592Z","shell.execute_reply.started":"2025-06-18T06:48:48.430406Z","shell.execute_reply":"2025-06-18T06:48:48.449814Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training**\n\nHere we train 16 models for 16 different horizons. We use three booting models: XG boost, light gradient boosting model, and cat boost - and we average these with weights that are related with the validation score of the models.","metadata":{}},{"cell_type":"code","source":"# Training xgboost. We do one split per type of model, to save time.\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y)\n\nmodels_xgb = {}\nweights_xgb = []\nfor i in range(1,17):\n    print(i)\n    model = XGBRegressor(\n        **best_params_xgb,\n        n_estimators=1000,\n        tree_method='hist',     \n        device = 'cuda',\n        early_stopping_rounds=5,\n        eval_metric='rmse',\n        enable_categorical = True\n    )\n    model.fit(\n        X_train[feature_list[i-1]], \n        y_train[f'log_sales_step_{i}'], \n        eval_set=[(X_valid[feature_list[i-1]], y_valid[f'log_sales_step_{i}'])], \n        verbose = 0)\n    preds = model.predict(X_valid[feature_list[i-1]])\n    weights_xgb.append(1/mean_squared_error(y_valid[f'log_sales_step_{i}'],preds))\n    models_xgb[i] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:48:48.451386Z","iopub.execute_input":"2025-06-18T06:48:48.451657Z","iopub.status.idle":"2025-06-18T07:05:21.474776Z","shell.execute_reply.started":"2025-06-18T06:48:48.451627Z","shell.execute_reply":"2025-06-18T07:05:21.474030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train lightgbm. We do one split per type of model, to save time.\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y)\n\nmodels_lgbm = {}\nweights_lgbm = []\nfor i in range(1, 17):\n    print(i)\n    model = LGBMRegressor(\n        **best_params_lgbm,    \n        device='gpu',           \n        gpu_platform_id=0,      \n        gpu_device_id=0, \n        n_estimators=1000\n    )\n    model.fit(\n        X_train[feature_list[i - 1]],\n        y_train[f'log_sales_step_{i}'],\n        eval_set=[(X_valid[feature_list[i - 1]], y_valid[f'log_sales_step_{i}'])],\n        callbacks=[early_stopping(stopping_rounds=5)],\n        eval_metric='rmse'\n    )\n    preds = model.predict(X_valid[feature_list[i-1]])\n    weights_lgbm.append(1/mean_squared_error(y_valid[f'log_sales_step_{i}'],preds))\n    models_lgbm[i] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T07:05:21.475538Z","iopub.execute_input":"2025-06-18T07:05:21.475808Z","iopub.status.idle":"2025-06-18T07:38:06.525404Z","shell.execute_reply.started":"2025-06-18T07:05:21.475787Z","shell.execute_reply":"2025-06-18T07:38:06.524394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train catboost. We do one split per type of model, to save time.\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y)\n\nmodels_cat = {}\nweights_cat = []\nfor i in range(1,17):\n    print(i)\n    model = CatBoostRegressor(\n        **best_params_cat,\n        loss_function='RMSE',\n        task_type=\"GPU\",        # <-- enable GPU\n        devices='0',            # use first GPU\n        iterations=1000\n    )\n    model.fit(\n        X_train[feature_list[i-1]],\n        y_train[f'log_sales_step_{i}'],\n        cat_features = list(set(categorical_features+future_categorical_features) & set(feature_list[i-1])),\n        eval_set=(X_valid[feature_list[i-1]], y_valid[f'log_sales_step_{i}']),\n        early_stopping_rounds = 5,\n        verbose = 50)\n    preds = model.predict(X_valid[feature_list[i-1]])\n    weights_cat.append(1/mean_squared_error(y_valid[f'log_sales_step_{i}'],preds))\n    models_cat[i] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T07:38:06.526332Z","iopub.execute_input":"2025-06-18T07:38:06.526647Z","iopub.status.idle":"2025-06-18T11:51:47.557773Z","shell.execute_reply.started":"2025-06-18T07:38:06.526619Z","shell.execute_reply":"2025-06-18T11:51:47.556905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(weights_xgb)\nprint(weights_lgbm)\nprint(weights_cat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:51:47.558750Z","iopub.execute_input":"2025-06-18T11:51:47.559070Z","iopub.status.idle":"2025-06-18T11:51:47.564423Z","shell.execute_reply.started":"2025-06-18T11:51:47.559039Z","shell.execute_reply":"2025-06-18T11:51:47.563619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# computing the weights for the weighted average\n\nweights_xgb = np.array(weights_xgb)\nweights_lgbm = np.array(weights_lgbm)\nweights_cat = np.array(weights_cat)\n\nsum_weights = weights_xgb+weights_lgbm+weights_cat\n\nweights_xgb = weights_xgb/sum_weights\nweights_lgbm = weights_lgbm/sum_weights\nweights_cat = weights_cat/sum_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:51:47.565165Z","iopub.execute_input":"2025-06-18T11:51:47.565353Z","iopub.status.idle":"2025-06-18T11:51:47.582127Z","shell.execute_reply.started":"2025-06-18T11:51:47.565336Z","shell.execute_reply":"2025-06-18T11:51:47.581337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict results on training set.\n\ncomparison_df = full_df.loc[(full_df['year']==2017) & (full_df['date'] <= prediction_date),:]\n\nfor model_type in ('xgb', 'lgbm', 'cat'):\n    if model_type == 'xgb':\n        models = models_xgb\n    elif model_type == 'lgbm':\n        models = models_lgbm\n    elif model_type == 'cat':\n        models = models_cat\n    for i in models.keys():\n        print(i)\n        y = models[i].predict(comparison_df[feature_list[i-1]])\n        y = np.exp(y)-1\n        y = np.clip(y, a_min = 0, a_max=None)\n        comparison_df[f'{model_type}_pred_sales_step_{i}'] = y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:51:47.582926Z","iopub.execute_input":"2025-06-18T11:51:47.583150Z","iopub.status.idle":"2025-06-18T11:58:00.677806Z","shell.execute_reply.started":"2025-06-18T11:51:47.583131Z","shell.execute_reply":"2025-06-18T11:58:00.676674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"comparison_df.to_csv('/kaggle/working/comparison_df.csv',index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:58:00.678735Z","iopub.execute_input":"2025-06-18T11:58:00.679067Z","iopub.status.idle":"2025-06-18T11:59:01.160947Z","shell.execute_reply.started":"2025-06-18T11:58:00.679039Z","shell.execute_reply":"2025-06-18T11:59:01.160061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Average predictions weighted by the performance-based weights\n\nfor i in range(1,17):\n    comparison_df[f'avg_pred_sales_step_{i}'] = comparison_df[[f'xgb_pred_sales_step_{i}',\n                                                              f'lgbm_pred_sales_step_{i}',\n                                                              f'cat_pred_sales_step_{i}']].mean(axis=1)\n    comparison_df[f'w_avg_pred_sales_step_{i}'] = weights_xgb[i-1]*comparison_df[f'xgb_pred_sales_step_{i}']+ weights_lgbm[i-1]*comparison_df[f'lgbm_pred_sales_step_{i}']+ weights_cat[i-1]*comparison_df[f'cat_pred_sales_step_{i}']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:01.161746Z","iopub.execute_input":"2025-06-18T11:59:01.161974Z","iopub.status.idle":"2025-06-18T11:59:02.046643Z","shell.execute_reply.started":"2025-06-18T11:59:01.161948Z","shell.execute_reply":"2025-06-18T11:59:02.045556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Examples\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 150)\n\ncomparison_df.head()\ncomparison_df[comparison_df['date']=='2017-08-22'].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:02.047546Z","iopub.execute_input":"2025-06-18T11:59:02.047873Z","iopub.status.idle":"2025-06-18T11:59:02.113362Z","shell.execute_reply.started":"2025-06-18T11:59:02.047839Z","shell.execute_reply":"2025-06-18T11:59:02.112743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following cell sets identifies stores and families that didn't sell any product during 2017, and then sets the predictions for these time-series to 0. The prediction should give negligible predictios anyway, but the real values are most likely 0 so we use that.","metadata":{}},{"cell_type":"code","source":"# Predict 0 if there were no sales in 2017 for a store_nbr, family\n\nno_sales_2017_df = comparison_df.groupby(['store_nbr', 'family'])[['sales']].sum()\nno_sales_2017_df = no_sales_2017_df.reset_index()\nno_sales_2017_df = no_sales_2017_df.rename(columns={'sales':'sum_2017_sales'})\nno_sales_2017_df = no_sales_2017_df[no_sales_2017_df['sum_2017_sales']==0]\n\ncomparison_df = pd.merge(left=comparison_df,\n                         right=no_sales_2017_df[['store_nbr','family','sum_2017_sales']],\n                         on=['store_nbr','family'],\n                         how='left')\n\nfor pred_type in ('avg', 'w_avg', 'xgb', 'lgbm', 'cat'):\n    for i in range(1,17):\n        comparison_df[f'{pred_type}_pred_sales_step_{i}'] = comparison_df['sum_2017_sales'].fillna(comparison_df[f'{pred_type}_pred_sales_step_{i}'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:02.202673Z","iopub.execute_input":"2025-06-18T11:59:02.202913Z","iopub.status.idle":"2025-06-18T11:59:02.807120Z","shell.execute_reply.started":"2025-06-18T11:59:02.202893Z","shell.execute_reply":"2025-06-18T11:59:02.806376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transpose predictions and join to test_df\n\nprediction_df = comparison_df[comparison_df['date']== prediction_date]\nprediction_df = prediction_df[['store_nbr', 'family'] \n                                +[f'avg_pred_sales_step_{i}' for i in range(1,17)]\n                                +[f'w_avg_pred_sales_step_{i}' for i in range(1,17)]\n                                +[f'xgb_pred_sales_step_{i}' for i in range(1,17)]\n                                +[f'lgbm_pred_sales_step_{i}' for i in range(1,17)]\n                                +[f'cat_pred_sales_step_{i}' for i in range(1,17)]]\n\n# Merging predicted sales data from prediction_date\nresult_df = test_df.reset_index()\nresult_df = pd.merge(left=result_df,\n                     right=prediction_df,\n                     on=['store_nbr', 'family'],how='left')\n\n# Choosing i-th prediction on day i + prediction_date\nresult_df['difference'] = (result_df['date']-pd.to_datetime(prediction_date)).dt.days\n\nfor pred_type in ('avg', 'w_avg', 'xgb', 'lgbm', 'cat'):\n    col_names = result_df['difference'].astype(str).radd(f'{pred_type}_pred_sales_step_')\n    col_codes, unique_cols = pd.factorize(col_names)\n    matrix = result_df[unique_cols].to_numpy()\n    result_df[f'{pred_type}_pred_sales'] = matrix[np.arange(len(result_df)), col_codes]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:02.807884Z","iopub.execute_input":"2025-06-18T11:59:02.808124Z","iopub.status.idle":"2025-06-18T11:59:02.911728Z","shell.execute_reply.started":"2025-06-18T11:59:02.808091Z","shell.execute_reply":"2025-06-18T11:59:02.911016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Examples**","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 150)\n\nresult_df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:02.912529Z","iopub.execute_input":"2025-06-18T11:59:02.912772Z","iopub.status.idle":"2025-06-18T11:59:02.964295Z","shell.execute_reply.started":"2025-06-18T11:59:02.912753Z","shell.execute_reply":"2025-06-18T11:59:02.963433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"str_nbr = 10\nfml = 'BEVERAGES'\n\ntrain_condition = (full_df['store_nbr']==str_nbr) & (full_df['family']==fml)\ntest_condition = (result_df['store_nbr']==str_nbr) & (result_df['family']==fml)\n\nplt.figure()\naxs = full_df.loc[train_condition,:].plot(x='date', y='sales_step_1',xlim = ('2017-07-01','2017-08-31')) #, ylim=(0,20)\naxs = result_df.loc[test_condition,:].plot(x='date', y='avg_pred_sales_step_1', ax=axs)\n#axs = full_df.loc[train_condition,:].plot(x='date', y='MA_last_month', ax=axs) #\n#axs = result_df.loc[test_condition,:].plot(x='date', y='predicted_sales', ax=axs) #\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:08.206203Z","iopub.execute_input":"2025-06-18T11:59:08.206432Z","iopub.status.idle":"2025-06-18T11:59:08.430931Z","shell.execute_reply.started":"2025-06-18T11:59:08.206411Z","shell.execute_reply":"2025-06-18T11:59:08.429901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluation**\n\nHere, we compute the F-score of different features to help understand the contribution of each feature to the performance. It helps us understand if any features are redundant, which helps simplify the model and avoid overfitting. It also reinforces our trust in the model if we believe that the order of feature importance makes sense.","metadata":{}},{"cell_type":"code","source":"from xgboost import plot_importance\n\nfor i in range(1,17):\n    model = models_xgb[i]\n    plt.figure()\n    plot_importance(model, title=f\"Target {i} Feature Importance\", max_num_features=30) # \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:08.432023Z","iopub.execute_input":"2025-06-18T11:59:08.432383Z","iopub.status.idle":"2025-06-18T11:59:14.946102Z","shell.execute_reply.started":"2025-06-18T11:59:08.432347Z","shell.execute_reply":"2025-06-18T11:59:14.945250Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Submission**\n\nThe rest of the code is just the organization of the prediction in a format suitable for submission.","metadata":{}},{"cell_type":"code","source":"result_df = result_df[['id','date','store_nbr','family','avg_pred_sales','w_avg_pred_sales','xgb_pred_sales','lgbm_pred_sales','cat_pred_sales']]\nresult_df.to_csv('/kaggle/working/result_df.csv',index=True)\nresult_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:14.947119Z","iopub.execute_input":"2025-06-18T11:59:14.947381Z","iopub.status.idle":"2025-06-18T11:59:15.190292Z","shell.execute_reply.started":"2025-06-18T11:59:14.947361Z","shell.execute_reply":"2025-06-18T11:59:15.189415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check submission file\n\nchosen_pred = 'cat_pred_sales' # options: 'avg_pred_sales', 'w_avg_pred_sales','xgb_pred_sales','lgbm_pred_sales','cat_pred_sales'\n\nto_submit = result_df[['id', chosen_pred]].rename(columns={chosen_pred:'sales'})\nto_submit.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:04:06.100933Z","iopub.execute_input":"2025-06-18T12:04:06.101215Z","iopub.status.idle":"2025-06-18T12:04:06.110953Z","shell.execute_reply.started":"2025-06-18T12:04:06.101193Z","shell.execute_reply":"2025-06-18T12:04:06.110237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Submission\n\nto_submit.to_csv('/kaggle/working/submission.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T12:04:12.652502Z","iopub.execute_input":"2025-06-18T12:04:12.652873Z","iopub.status.idle":"2025-06-18T12:04:12.705796Z","shell.execute_reply.started":"2025-06-18T12:04:12.652843Z","shell.execute_reply":"2025-06-18T12:04:12.704912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Submission using predictions-df**","metadata":{}},{"cell_type":"code","source":"# # Run first 5 cells. Then:\n\n# predictions_df = pd.read_csv('/kaggle/input/predictions-df/comparison_df (2).csv')\n\n# pd.set_option('display.max_rows', 500)\n# pd.set_option('display.max_columns', 500)\n# pd.set_option('display.width', 150)\n\n# predictions_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:15.426349Z","iopub.execute_input":"2025-06-18T11:59:15.426610Z","iopub.status.idle":"2025-06-18T11:59:15.429569Z","shell.execute_reply.started":"2025-06-18T11:59:15.426564Z","shell.execute_reply":"2025-06-18T11:59:15.428775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # No weights available, so excluded.\n\n# for i in range(1,17):\n#     predictions_df[f'avg_pred_sales_step_{i}'] = predictions_df[[f'xgb_pred_sales_step_{i}',\n#                                                               f'lgbm_pred_sales_step_{i}',\n#                                                               f'cat_pred_sales_step_{i}']].mean(axis=1)\n\n# # Transpose predictions and join to test_df\n\n# # Predict 0 if there were no sales in 2017 for a store_nbr, family\n\n# comparison_df = full_df.loc[full_df['year']==2017,:]\n# no_sales_2017_df = comparison_df.groupby(['store_nbr', 'family'])[['sales']].sum()\n# no_sales_2017_df = no_sales_2017_df.reset_index()\n# no_sales_2017_df = no_sales_2017_df.rename(columns={'sales':'sum_2017_sales'})\n# no_sales_2017_df = no_sales_2017_df[no_sales_2017_df['sum_2017_sales']==0]\n\n# predictions_df = pd.merge(left=predictions_df,\n#                          right=no_sales_2017_df[['store_nbr','family','sum_2017_sales']],\n#                          on=['store_nbr','family'],\n#                          how='left')\n\n# for pred_type in ('avg', 'xgb', 'lgbm', 'cat'):\n#     for i in range(1,17):\n#         predictions_df[f'{pred_type}_pred_sales_step_{i}'] = predictions_df['sum_2017_sales'].fillna(predictions_df[f'{pred_type}_pred_sales_step_{i}'])\n\n# #prediction_df = comparison_df[comparison_df['date']== prediction_date]\n# predictions_df = predictions_df[['store_nbr', 'family'] \n#                                 +[f'avg_pred_sales_step_{i}' for i in range(1,17)]\n#                                 +[f'xgb_pred_sales_step_{i}' for i in range(1,17)]\n#                                 +[f'lgbm_pred_sales_step_{i}' for i in range(1,17)]\n#                                 +[f'cat_pred_sales_step_{i}' for i in range(1,17)]]\n\n# # Merging predicted sales data from prediction_date\n# result_df = test_df.reset_index()\n# result_df = pd.merge(left=result_df,\n#                      right=predictions_df,\n#                      on=['store_nbr', 'family'],how='left')\n\n# # Choosing i-th prediction on day i + prediction_date\n# result_df['difference'] = (result_df['date']-pd.to_datetime(prediction_date)).dt.days\n\n# for pred_type in ('avg', 'xgb', 'lgbm', 'cat'):\n#     col_names = result_df['difference'].astype(str).radd(f'{pred_type}_pred_sales_step_')\n#     col_codes, unique_cols = pd.factorize(col_names)\n#     matrix = result_df[unique_cols].to_numpy()\n#     result_df[f'{pred_type}_pred_sales'] = matrix[np.arange(len(result_df)), col_codes]\n\n# result_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:15.430261Z","iopub.execute_input":"2025-06-18T11:59:15.430448Z","iopub.status.idle":"2025-06-18T11:59:15.450929Z","shell.execute_reply.started":"2025-06-18T11:59:15.430432Z","shell.execute_reply":"2025-06-18T11:59:15.450138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check submission file\n\n# chosen_pred = 'lgbm_pred_sales' # options: 'avg_pred_sales', 'w_avg_pred_sales','xgb_pred_sales','lgbm_pred_sales','cat_pred_sales'\n\n# to_submit = result_df[['id', chosen_pred]].rename(columns={chosen_pred:'sales'})\n# to_submit.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:15.451696Z","iopub.execute_input":"2025-06-18T11:59:15.451912Z","iopub.status.idle":"2025-06-18T11:59:15.473654Z","shell.execute_reply.started":"2025-06-18T11:59:15.451893Z","shell.execute_reply":"2025-06-18T11:59:15.472925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Submission\n\n# to_submit.to_csv('/kaggle/working/submission.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T11:59:15.474608Z","iopub.execute_input":"2025-06-18T11:59:15.474833Z","iopub.status.idle":"2025-06-18T11:59:15.489353Z","shell.execute_reply.started":"2025-06-18T11:59:15.474813Z","shell.execute_reply":"2025-06-18T11:59:15.488788Z"}},"outputs":[],"execution_count":null}]}